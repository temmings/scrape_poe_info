#! python3
"""
# scrape_poe_maps.py - scrapes poe maps from http://pathofexile.gamepedia.com/Map
# (modified from scrape_poe_uniques.py)
"""

import requests, bs4, re, datetime, time, json, os
from bs4 import NavigableString
from multiprocessing.dummy import Pool as ThreadPool

SCRIPTDIR = os.path.dirname(os.path.abspath(__file__))

base_url = 'http://pathofexile.gamepedia.com'
main_url = base_url + '/User:Temmings/MapData'

rx_search = re.compile(r'\+*\(([\d\.]+)\s[a-z]+\s([\d\.]+)[)]|(\+*[\d\.]+\%)|([\d\.]+-[\d\.]+)|(\([\d\.]+-[\d\.]+)\s\w+\s([\d\.]+-[\d\.]+\))|(-?\+?[\d\.]+)')
vendor_regex = re.compile('yields? one|produces? one', re.IGNORECASE)
maptype_regex = re.compile('Map type', re.IGNORECASE)


def write_file_headers():
	"""
	info headers for MapList.txt

	:return: list
	"""
	data = []
	d = datetime.datetime.now()
	now_time = d.strftime('%Y-%m-%d at %H:%M:%S')
	data.append('; Data from ' + main_url)
	data.append('; Comments can be made with ";", blank lines will be ignored.')
	data.append(';')
	data.append('; This file was auto-generated by scrape_poe_maps.py on {}'.format(now_time) + '\n')
	data.append('mapList := Object()')
	data.append('mapList["Unknown Map"] := "Map not recognised or not supported"\n')
	data.append('uniqueMapList := Object()')
	data.append('uniqueMapList["Unknown Map"] := "Map not recognised or not supported"\n')

	return data


def get_main_page(url):
	"""
	Gets the main wiki page for Maps and parses out the links for each map
	and returns a list of the urls
	:param url: main page url
	:return: list, containing basic map data and list of urls
	"""
	map_list = []
	print('Getting User:Temmings/MapData ...')
	page = requests.get(url)
	page.raise_for_status()
	soup = bs4.BeautifulSoup(page.text, 'html.parser')
	map_table = soup.find_all('table', class_='wikitable sortable')
	mapcount = 1
	for row in map_table[0].find_all('tr'):
		tds = row.find_all('td')
		if len(tds) == 0:	# exclude header row
			continue
		map_info = {}
		map_info['count'] = mapcount
		mapcount += 1
		map_info['tier'] = tds[0].text.strip()
		map_info['level'] = tds[1].text.strip()
		map_info['name'] = tds[2].text.strip()
		map_info['url'] = base_url + '/' + map_info['name'].replace(' ', '_') + '_Map_(War_for_the_Atlas)'
		map_info['producedby'] = tds[3].text.strip().replace(';', ', ')
		map_info['upgradesto'] = tds[4].text.strip()
		map_info['tileset'] = tds[5].text.strip()
		map_info['unique'] = False
		
		map_list.append(map_info)
	
	for row in map_table[1].find_all('tr'):
		tds = row.find_all('td')
		if len(tds) == 0:	# exclude header row
			continue
		map_info = {}
		map_info['count'] = mapcount
		mapcount += 1
		map_info['tier'] = tds[0].text.strip()
		map_info['level'] = tds[1].text.strip()
		map_info['name'] = tds[2].text.strip()
		map_info['url'] = base_url + '/' + map_info['name'].replace(' ', '_') + '_(War_for_the_Atlas)'
		#map_info['base'] = tds[3].text.strip()
		map_info['tileset'] = tds[4].text.strip()
		map_info['unique'] = True
		
		map_list.append(map_info)

	return map_list


def parse_map_data(map_info):
	"""
	fetches the page for a map
	:param links:
	:return:
	"""
	page = requests.get(map_info['url'])
	page.raise_for_status()
	soup = bs4.BeautifulSoup(page.text, 'html.parser')
	return build_data(soup, map_info)

"""
def find_divcards(div):
	for h2 in div.find_all('h2'):
		if h2.find('span', id='Divination_cards'):
			return h2
	return None
	
def find_vendor_recipe(div):
	try:
		for yields in div.find_all(text=vendor_regex):
			return yields.next_sibling.findNext('a').findNext('a').text
	except:
		return None
	return None
	

def find_setting(div):
	try:
		for maptype in div.find_all(text=maptype_regex):
			return maptype.parent.next_sibling.replace(':', '').strip()
	except:
		return None
	return None
"""


def build_data(data, mapinfo):
	"""
	parse map data from the page
	:param data: BS4 ResultSet
	:return: list
	"""
	map_data = dict(mapinfo)
	print('Getting data for {}'.format(map_data['name']))
	
	map_data['divcards'] = []
	
	items_heading = data.find(id='Items_found_in_this_area')
	if items_heading is not None:
		items_table = items_heading.find_next('table')
		
		divcards = items_table.find_all('span', class_='divicard-header')
		for divcard in divcards:
			map_data['divcards'].append(divcard.text)
		
	# find the map setting (indoors/outdoors)
	#map_data['setting'] = find_setting(div)
	
	return map_data


def convert_data_to_AHK_readable_format(all_data):
	"""
	This function takes the raw web page data, and converts it into lines that are readable by the
	Poe_item_info AHK script.
	:return:
	"""
	
	# Read the descriptions of various maps
	with open(SCRIPTDIR + '\\MapDescriptions.json', 'r') as f:
		map_descriptions = json.load(f)
	
	uniqueMapNameFromBase = open(SCRIPTDIR + '\\MapNameFromBase.txt', 'r').read()
	
	new_data = []
	unique_map = {}
	matchList = []
	for mymap in all_data:
		if mymap['unique'] is False:
			matchList.append(mymap['name'] + ' Map')

	# lists sorted by descending name length to avoid mismatching ("Spider Lair Map" before "Lair Map" etc.)
	matchList.sort(key=len, reverse=True)
	
	new_data.append('mapMatchList := ["' + '","'.join(matchList) + '"]\n')
	
	new_data.append('\n' + uniqueMapNameFromBase + '\n')
	
	for mymap in all_data:
		
		line = ''
		#line = 'Tier: ' + mymap['tier'] + ', Level: ' + mymap['level']
		#line += '`nTileset: ' + mymap['tileset']
		#if mymap['setting'] is not None:
		#	line += ' (' + mymap['setting'] + ')'

		# Add on vendor recipes and connected maps
		if mymap['unique'] is False:
			vendor_lines = '3 to 1 vendor recipe:'
			if mymap['producedby']:
				vendor_lines += '`n Produced by: ' + mymap['producedby']
			else:
				vendor_lines += '`n Produced by: none'
			if mymap['upgradesto']:
				vendor_lines += '`n Upgrades to: ' + mymap['upgradesto']
			elif mymap['tier'] == '16':
				vendor_lines += '`n Upgrades to: none'
			else:
				vendor_lines += '`n Upgrades to: ?'
			
			line += vendor_lines
		
		# Add line when map is shaped
		#if mymap['shaped'] == 'yes':
		#	line += '`n`nInfos from ' + mymap['base'] + ':'

		# Add on unique version if one exists
		#if mymap['base'] in unique_map and mymap['unique'] == 'no':
		#	line += '`n`nUnique version of map: ' + unique_map[mymap['base']]

		# Add on divination cards
		if len(mymap['divcards']) > 0:
			line += '`n`nDivination cards:'
			for divcard in mymap['divcards']:
				line += '`n ' + divcard
				
		# Here we insert the prewritten text descriptions for the maps that have them
		if mymap['unique']:
			if mymap['name'] in map_descriptions['uniqueMaps']:
				line += '`n`n' + map_descriptions['uniqueMaps'][mymap['name']]
		else:
			if mymap['name'] in map_descriptions['maps']:
				line += '`n`n' + map_descriptions['maps'][mymap['name']]

		line = line.lstrip('`n')
		
		if mymap['unique']:
			entry = 'uniqueMapList["' + mymap['name'] + '"] := "' + line + '"\n'
		else:
			entry = 'mapList["' + mymap['name'] + ' Map"] := "' + line + '"\n'
		
		new_data.append(entry)
	
	return new_data


def write(new_data):
	file = open(SCRIPTDIR + '\\MapList.txt', 'a+b')  # opens file for writing
	for row in new_data:
		file.write(row.encode('utf8'))
		file.write(b'\n')
	file.close()


def main():
	map_list = get_main_page(main_url)
	open(SCRIPTDIR + '\\MapList.txt', 'w').close()  # create file (or overwrite it if it exists)
	write(write_file_headers())
	pool = ThreadPool(4)
	data = pool.map(parse_map_data, map_list)
	data.sort(key=lambda m: m['count'])
	x = convert_data_to_AHK_readable_format(data)
	write(x)
	pool.close()
	pool.join()


startTime = datetime.datetime.now()
main()
print('Program execution time: ',(datetime.datetime.now() - startTime))
